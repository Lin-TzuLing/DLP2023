{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7069cde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin/Desktop/dlp_final/PRTR/two_stage/tools/../lib/models/__init__.py:16: UserWarning: No module named 'MultiScaleDeformableAttention'. deformable_pose_transformer will be unavailable\n",
      "  warnings.warn(f'{ex}. deformable_pose_transformer will be unavailable')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from tools import _init_paths\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = 10.0, 8.0\n",
    "\n",
    "import dataset\n",
    "import models\n",
    "from config import cfg as conf\n",
    "from config import update_config\n",
    "from utils.utils import model_key_helper\n",
    "from core.inference import get_final_preds_match\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca234ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    SKEL = [[16, 14], [14, 12], [17, 15], [15, 13], [12, 13], [6, 12], [7, 13], \n",
    "            [6, 7], [6, 8], [7, 9], [8, 10], [9, 11], [2, 3], [1, 2], [1, 3], \n",
    "            [2, 4], [3, 5], [4, 6], [5, 7]]\n",
    "\n",
    "    def plot_results(self, img, pred, vis, scale):\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        GREEN = [(5,6),(6,8),(8,10)]\n",
    "        YELLOW = [(5,7),(7,9)]\n",
    "        BLUE = [(5,11),(11,13),(13,15)]\n",
    "        PINK = [(6,12),(12,14),(14,16)]\n",
    "\n",
    "        vis_head = pred[:5][vis[:5] > 0]\n",
    "        vis_body = pred[5:][vis[5:] > 0]\n",
    "\n",
    "        for i, j in self.SKEL:\n",
    "            ki = min(i, j) - 1\n",
    "            kj = max(i, j) - 1\n",
    "            # 0-4 head\n",
    "            # 5,7,9,11,13,15 left\n",
    "            # 6,8,10,12,14,16 right\n",
    "\n",
    "            i, j = i - 1, j - 1\n",
    "            src = pred[i]\n",
    "            dst = pred[j]\n",
    "            if vis[i] <= 0 or vis[j] <= 0:\n",
    "                continue\n",
    "            # face detail\n",
    "            if ki < 5 and kj < 5:\n",
    "                plt.plot([src[0], dst[0]],[src[1], dst[1]], linewidth=100, color=[250/255, 32/255, 98/255, 1], \n",
    "                        solid_capstyle='round', zorder=1)\n",
    "            if ki >=5 and kj >= 5:\n",
    "                pair = ki, kj\n",
    "                trans = 1\n",
    "                if pair in GREEN:\n",
    "                    color = (38 / 255, 252 / 255, 145 / 255, trans)\n",
    "                elif pair in YELLOW:\n",
    "                    color = [250 / 255, 244 / 255, 60 / 255, trans]\n",
    "                elif pair in BLUE:\n",
    "                    color = [104 / 255, 252 / 255, 252 / 255, trans]\n",
    "                elif pair in PINK:\n",
    "                    color = [255 / 255, 148 / 255, 212 / 255, trans]\n",
    "                else:\n",
    "                    continue\n",
    "                plt.plot([src[0], dst[0]],[src[1], dst[1]], linewidth=6.0 * scale, color=color, \n",
    "                         solid_capstyle='round', zorder=1)\n",
    "\n",
    "            for pnt in vis_head:\n",
    "                circle = plt.Circle((pnt[0], pnt[1]), 1.5 * scale * 1.2, fill=False, color='black', zorder=10)\n",
    "                ax.add_artist(circle)\n",
    "            for pnt in vis_body:\n",
    "                circle = plt.Circle((pnt[0], pnt[1]), 3.0 * scale * 1.2, fill=False, color='black', zorder=10)\n",
    "                ax.add_artist(circle)\n",
    "\n",
    "            \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Args:\n",
    "    cfg = 'experiments/coco/transformer/w32_384x288_adamw_lr1e-4.yaml'\n",
    "    opts = []\n",
    "    modelDir = None\n",
    "    logDir = None\n",
    "    dataDir = None\n",
    "    pretrained = 'lib/models/pytorch/pose_coco/pose_transformer_hrnet_w32_384x288.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b16927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(config, flip_pairs, input, size, target=None, target_weight=None, meta=None, transform=None):\n",
    "    #c = meta['center']\n",
    "    #s = meta['scale']\n",
    "    outputs = model(transform(input)[None, ...])\n",
    "    preds, _, preds_raw, preds_raw_ = get_final_preds_match(config, outputs, \n",
    "                                                np.array([size[0]/2,size[1]/2], dtype=np.float32), \n",
    "                                                np.array([2.,2.], dtype=np.float32))\n",
    "\n",
    "    vis = np.ones((17,))\n",
    "    # ignore face detail     \n",
    "    vis[1:5] = 0.\n",
    "    # vis[0] = 1.\n",
    "    \n",
    "    return preds_raw[0], vis, 1., preds_raw_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511486d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Found 470 images...\n",
      "> Found 1081 images...\n",
      "Training Loaded 470 images\n",
      "Testing Loaded 1081 images\n"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "update_config(conf, args)\n",
    "model = models.pose_transformer.get_pose_net(conf, is_train=False)\n",
    "model.load_state_dict(model_key_helper(torch.load(args.pretrained, map_location='cpu')))\n",
    "\n",
    "normalize = transforms.Compose([\n",
    "    transforms.Resize((384, 288)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# use original TRAIN dataset as testing dataset, TEST dataset as training dataset\n",
    "train_dataset = dataset.yoga(\n",
    "    root = \"/home/lin/Desktop/dlp_final/DATASET/TEST\", mode = \"train\", size=(384, 288)\n",
    ")\n",
    "valid_dataset = dataset.yoga(\n",
    "    root = \"/home/lin/Desktop/dlp_final/DATASET/TRAIN\", mode = \"test\", size=(384, 288)\n",
    ")\n",
    "\n",
    "\n",
    "#valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "print(f\"Training Loaded {len(train_dataset)} images\")\n",
    "print(f\"Testing Loaded {len(valid_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a276de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIS_INDICES = [300, 100]\n",
    "keypoint_dict = {0:\"nose\", 1:\"left_eye\", 2:\"right_eye\", 3:\"left_ear\", 4:\"right_ear\", 5:\"left_shoulder\",\n",
    "                 6:\"right_shoulder\", 7:\"left_elbow\", 8:\"right_elbow\", 9:\"left_wrist\", 10:\"right_wrist\", \n",
    "                 11:\"left_hip\", 12:\"right_hip\", 13:\"left_knee\", 14:\"right_knee\", 15:\"left_ankle\", 16:\"right_ankle\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21b22039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_min_max(df):\n",
    "    df['x-coord norm'] = df['x-coord']\n",
    "    df['y-coord norm'] = df['y-coord']\n",
    "    df['x-coord norm'] = (df['x-coord norm'] - df['x-coord norm'].min()) / (df['x-coord norm'].max() - df['x-coord norm'].min())  \n",
    "    df['y-coord norm'] = (df['y-coord norm'] - df['y-coord norm'].min()) / (df['y-coord norm'].max() - df['y-coord norm'].min())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c05a11e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_keypoint(dataset):\n",
    "    for image_idx in tqdm(range(len(dataset))):\n",
    "\n",
    "        # get data\n",
    "        input, label, w, h, path = dataset[image_idx]\n",
    "\n",
    "        # inference, \n",
    "        # preds_raw: PRTR output after scaling, [keypoints_id, x-coord, y-coord]\n",
    "        # preds_raw_:PRTR direct output, range:[0,1]\n",
    "        preds_raw, vis, scale, preds_raw_= inference(conf, valid_dataset.flip_pairs, input, size=(w, h),\n",
    "                                                 target=None, target_weight=None, meta=None, transform=normalize)\n",
    "\n",
    "        # coordinate normalization\n",
    "        data = {'keypoint id': torch.arange(0,17),\n",
    "                'keypoint name': keypoint_dict.values(),\n",
    "                'vis': vis,\n",
    "                'x-coord': preds_raw_[:,0],\n",
    "                'y-coord': preds_raw_[:,1]\n",
    "               }\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # only want 13 keypoint\n",
    "        exclude_keypoint = [\"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\"]\n",
    "#         12 keypoint\n",
    "#         exclude_keypoint = [\"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\", \"nose\"]\n",
    "        keypoint_df = df[~df['keypoint name'].isin(exclude_keypoint)].reset_index(drop=True)\n",
    "        # keypoint_df = normalize_min_max(keypoint_df)\n",
    "\n",
    "        # save keypoint\n",
    "        tmp = []\n",
    "        # tmp.append(torch.from_numpy(keypoint_df['x-coord norm'].values))\n",
    "        # tmp.append(torch.from_numpy(keypoint_df['y-coord norm'].values))\n",
    "        tmp.append(torch.from_numpy(keypoint_df['x-coord'].values))\n",
    "        tmp.append(torch.from_numpy(keypoint_df['y-coord'].values))\n",
    "        tmp = torch.stack(tmp).transpose(0,1).numpy()\n",
    "\n",
    "        # print(preds_raw_)\n",
    "        np.savetxt(path.split('.')[0]+'.txt', tmp)\n",
    "\n",
    "        # visualization \n",
    "        # Visualizer().plot_results(input, preds_raw, vis, scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ec7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 470/470 [01:28<00:00,  5.32it/s]\n"
     ]
    }
   ],
   "source": [
    "save_keypoint(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0442c307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████                                                                           | 93/1081 [00:18<03:03,  5.39it/s]/home/lin/.virtualenvs/final/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 1081/1081 [03:29<00:00,  5.16it/s]\n"
     ]
    }
   ],
   "source": [
    "save_keypoint(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623368f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
